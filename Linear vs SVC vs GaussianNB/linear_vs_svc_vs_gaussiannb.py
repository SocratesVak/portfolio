# -*- coding: utf-8 -*-
"""linear vs SVC vs GaussianNB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MGvwfvGICbzf16vDv79sEEOBWf7u53aI
"""

import numpy as np
import pandas as pd
from sklearn import linear_model,metrics
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC # "Support vector classifier"
from sklearn.naive_bayes import GaussianNB

from google.colab import drive
drive.mount('/content/drive')

import os 
os.chdir('/content/drive/MyDrive')
!pwd #εντολή που μας τυπώνει σε ποιο φάκελο κοιτάει ο υπολογιστής

# Import Data
filename = "./heart.csv"
data_multi = pd.read_csv(filename)
# εδώ δίνουμε εντολή αφαίρεσης ολόκληρης γραμμής, αν βρεθεί έστω και μια στήλη της γραμμής τιμή NaN
data_multi.dropna(inplace=True)


# μετατρέπει τα δεδομένα σε πίνακα διανυσμάτων
temp_multi = data_multi.to_numpy()
# πάρε όλες τις γραμμές και όλες τις στήλες πλήν της τελευταίας. Έτσι πάιρνουμε τον πίνακα χαρακτηριστικών μας
X_multi = temp_multi[:,:-1] 
# πάρε όλες τις γραμμές από την τελευταία στήλη. Έτσι παίρνουμε το διάνυσμα που περιλαμβάνει τα output values
y_multi = temp_multi[:,-1] 
# εδώ προσθέτουμε ως πρώτη στήλη γεμάτη με 1 τα Χ0 μας
X_multi = np.column_stack((np.ones((X_multi.shape[0])),X_multi))
print(X_multi.shape)

#εδώ χωρίζουμε τα δεδομένα μας σε train και test. H αναλογία που επελέχθη είναι train:90% - test:10%
X_train, X_test, y_train, y_test = train_test_split(X_multi, y_multi, test_size=0.1)

regr = linear_model.LogisticRegression(fit_intercept=False,max_iter=1000)

regr.fit(X_train,y_train)

y_pred = regr.predict(X_test)

print("Logistic Regression - no feature expansion - test set", '\n', '------------------------')

print("Accuracy: ",regr.score(X_test, y_test))

print("coefficients: ",regr.coef_)

print("Confusion Matrix: ",'\n',metrics.confusion_matrix(y_test, y_pred),"\n","----------------------------")

y_pred = regr.predict(X_train)

print("Logistic Regression - no feature expansion - train set", '\n', '------------------------')

print("Accuracy: ",regr.score(X_train, y_train))

print("coefficients: ",regr.coef_)

print("Confusion Matrix: ",'\n', metrics.confusion_matrix(y_train, y_pred),"\n","----------------------------")

poly = PolynomialFeatures(degree=3,include_bias=False)
X_train_trf = poly.fit_transform(X_train)
X_test_trf = poly.transform(X_test)

regr = linear_model.LogisticRegression(fit_intercept=False,max_iter=1000)

regr.fit(X_train_trf,y_train)

y_pred = regr.predict(X_test_trf)

print("Logistic Regression - with feature expansion - test set", '\n', '------------------------')

print("Accuracy: ",regr.score(X_test_trf, y_test))

#print("coefficients: ",regr.coef_)

print("Confusion Matrix: ",'\n',metrics.confusion_matrix(y_test, y_pred),"\n","----------------------------")

y_pred = regr.predict(X_train_trf)

print("Logistic Regression - with feature expansion - train set", '\n', '------------------------')

print("Accuracy: ",regr.score(X_train_trf, y_train))

#print("coefficients: ",regr.coef_)

print("Confusion Matrix: ",'\n',metrics.confusion_matrix(y_train, y_pred),"\n","----------------------------")

model = SVC(kernel='linear', C=1)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("for linear kernel with C=1 in test set: ","\n","----------------------------")
print("Accuracy: ",metrics.accuracy_score(y_test,y_pred))
print("Coefficients: ",model.coef_)
print("Confusion Matrix: ",'\n',metrics.confusion_matrix(y_test, y_pred),"\n","----------------------------")

model = SVC(kernel='linear', C=1)
model.fit(X_train, y_train)
y_pred = model.predict(X_train)

print("for linear kernel with C=1 in training set: ","\n","----------------------------")
print("Accuracy: ",metrics.accuracy_score(y_train,y_pred))
print("Coefficients: ",model.coef_)
print("Confusion Matrix: ",'\n',metrics.confusion_matrix(y_train, y_pred),"\n","----------------------------")

model = SVC(kernel='linear', C=10)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("for linear kernel with C=10 in test set: ","\n","----------------------------")
print("Accuracy: ",metrics.accuracy_score(y_test,y_pred))
print("Coefficients: ",model.coef_)
print("Confusion Matrix: ",'\n',metrics.confusion_matrix(y_test, y_pred),"\n","----------------------------")

model = SVC(kernel='linear', C=10)
model.fit(X_train, y_train)
y_pred = model.predict(X_train)

print("for linear kernel with C=10 in training set:","\n","----------------------------")
print("Accuracy: ",metrics.accuracy_score(y_train,y_pred))
print("Coefficients: ",model.coef_)
print("Confusion Matrix: ",'\n',metrics.confusion_matrix(y_train, y_pred),"\n","----------------------------")

model = SVC(kernel='linear', C=20)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("for linear kernel with C=20 in test set: ","\n","----------------------------")
print("Accuracy: ",metrics.accuracy_score(y_test,y_pred))
print("Coefficients: ",model.coef_)
print("Confusion Matrix: ",'\n',metrics.confusion_matrix(y_test, y_pred),"\n","----------------------------")

model = SVC(kernel='linear', C=20)
model.fit(X_train, y_train)
y_pred = model.predict(X_train)

print("for linear kernel with C=20 in training set: ","\n","----------------------------")
print("Accuracy: ",metrics.accuracy_score(y_train,y_pred))
print("Coefficients: ",model.coef_)
print("Confusion Matrix: ",'\n',metrics.confusion_matrix(y_train, y_pred),"\n","----------------------------")

model = SVC(kernel='rbf', C=1)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("for radial basis function kernel with C=1 in test set:","\n","----------------------------")
print("Accuracy: ",metrics.accuracy_score(y_test,y_pred))
print("Confusion Matrix: ",'\n',metrics.confusion_matrix(y_test, y_pred),"\n","----------------------------")

model = SVC(kernel='rbf', C=1)
model.fit(X_train, y_train)
y_pred = model.predict(X_train)

print("for radial basis function kernel with C=1 in training set:","\n","----------------------------")
print("Accuracy: ",metrics.accuracy_score(y_train,y_pred))
print("Confusion Matrix: ",'\n',metrics.confusion_matrix(y_train, y_pred),"\n","----------------------------")

model = SVC(kernel='rbf', C=10)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("for radial basis function kernel with C=10 in test set:","\n","----------------------------")
print("Accuracy: ",metrics.accuracy_score(y_test,y_pred))
print("Confusion Matrix: ",'\n',metrics.confusion_matrix(y_test, y_pred),"\n","----------------------------")

model = SVC(kernel='rbf', C=10)
model.fit(X_train, y_train)
y_pred = model.predict(X_train)

print("for radial basis function kernel with C=10 in training set:","\n","----------------------------")
print("Accuracy: ",metrics.accuracy_score(y_train,y_pred))
print("Confusion Matrix: ",'\n',metrics.confusion_matrix(y_train, y_pred),"\n","----------------------------")

model = SVC(kernel='rbf', C=20)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("for radial basis function kernel with C=20 in test set:","\n","----------------------------")
print("Accuracy: ",metrics.accuracy_score(y_test,y_pred))
print("Confusion Matrix: ",'\n',metrics.confusion_matrix(y_test, y_pred),"\n","----------------------------")

model = SVC(kernel='rbf', C=20)
model.fit(X_train, y_train)
y_pred = model.predict(X_train)

print("for radial basis function kernel with C=20 in training set:","\n","----------------------------")
print("Accuracy: ",metrics.accuracy_score(y_train,y_pred))
print("Confusion Matrix: ",'\n',metrics.confusion_matrix(y_train, y_pred),"\n","----------------------------")

gnb = GaussianNB()
gnb.fit(X_train,y_train)
y_pred = gnb.predict(X_test)

print("Naive Bayes in test set",'\n','---------------------')
print("Accuracy: ",metrics.accuracy_score(y_test,y_pred))
print("Confusion Matrix: ",'\n',metrics.confusion_matrix(y_test, y_pred),"\n","----------------------------")

gnb = GaussianNB()
gnb.fit(X_train,y_train)
y_pred = gnb.predict(X_train)

print("Naive Bayes in training set",'\n','---------------------')
print("Accuracy: ",metrics.accuracy_score(y_train,y_pred))
print("Confusion Matrix: ",'\n',metrics.confusion_matrix(y_train, y_pred),"\n","----------------------------")

"""1. Question 1 : b. ability to generalize well
   Question 2 : a. classification
   Question 3 : a. supervised learning

2. Μια γενική παρατήρηση είναι ότι λόγω του split (90 % training - 10 % test) που επελέχθη οι classifiers πετυχαίνουν κατά κανόνα υψηλότερα σκορ απόδοσης στο training set συγκριτικά με τα αντίστοιχα στο test set, γεγονός που μπορεί να ερμηνευτεί ως μια μορφή overfitting, υπερπροσαρμογής, δηλαδή, της μηχανής στα δεδομένα εκπαίδευσης και αδυναμίας γενικευτικής ανάλυσης.

Oι Logistic Regression classifier, Support Vector Classifier με linear kernel (C=1/C=10) και ο Naive Bayes διακρίνονται, καθώς αποδίδουν παρόμοια υψηλά accuracy score(π.χ. 0.87). Αυτό τεκμαίρεται και από την εικόνα που μας δίνουν τα confusion matrix, καθώς η διαγώνιος των πινάκων, που δείχνει τα σωστά ταξινομημένα δείγματα είναι πανομοιότυπη. Κάθε φορά που εκτελείται όλος ο παραπάνω κώδικας το προβάδισμα δίνεται κάθε φορά σε άλλον classifier από τους τρεις, οπότε αδυνατώ να ξεχωρίσω έναν ως τον πλέον κατάλληλο. 

O Support Vector Classifier με linear kernel (C=20) δείχνει να μην είναι τόσο σταθερός όσο οι linear kernel με μικρότερο C. Φαίνεται ότι το εν λόγω πρόβλημα προκρίνει να είμαστε πιο αυστηροί με τα δείγματα που αφήνουμε να βρεθούν μες στο margin. Και η ευαίσθητη ιατρική φύση του προβλήματος επιτάσσει κάτι τέτοιο άλλωστε.

Ο  Support Vector Classifier με radial basis function kernel σημειώνει σημαντικά χαμηλότερα απόδοση. Αυτή μπορεί να ερμηνευτεί ως αποτελέσμα του γεγονότος ότι ο radial kernel αποδίδει σε multiclass classification προβλήματα, ενώ στην περίπτωση μας έχουμε binary classification πρόβλημα. 


"""