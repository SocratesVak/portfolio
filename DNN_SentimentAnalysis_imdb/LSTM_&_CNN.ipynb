{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar xvzf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "id": "7IgnCs2Alh5-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "positive_samples = []\n",
        "positive_directory = '/content/aclImdb/train/pos/'\n",
        "for filename in os.listdir(positive_directory):\n",
        "    with open(os.path.join(positive_directory, filename), 'r') as file:\n",
        "        positive_samples.append(file.read())"
      ],
      "metadata": {
        "id": "c4z6L8_tuGyD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_directory = '/content/aclImdb/test/pos/'\n",
        "for filename in os.listdir(positive_directory):\n",
        "    with open(os.path.join(positive_directory, filename), 'r') as file:\n",
        "        positive_samples.append(file.read())"
      ],
      "metadata": {
        "id": "oMh5NUxTOuAc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_samples = []\n",
        "negative_directory = '/content/aclImdb/train/neg/'\n",
        "for filename in os.listdir(negative_directory):\n",
        "    with open(os.path.join(negative_directory, filename), 'r') as file:\n",
        "        negative_samples.append(file.read())"
      ],
      "metadata": {
        "id": "7pKxdsfIuJX3"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_directory = '/content/aclImdb/test/neg/'\n",
        "for filename in os.listdir(negative_directory):\n",
        "    with open(os.path.join(negative_directory, filename), 'r') as file:\n",
        "        negative_samples.append(file.read())"
      ],
      "metadata": {
        "id": "3mdNhyYzOzf3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "pos_texts= np.array(positive_samples)\n",
        "neg_texts= np.array(negative_samples)\n",
        "pos_labels=  np.array([1]*len(positive_samples))\n",
        "neg_labels=  np.array([0]*len(negative_samples))\n",
        "\n",
        "pos_dataset = pd.DataFrame({'review': pos_texts, 'label': pos_labels}, columns=['review', 'label'])\n",
        "neg_dataset = pd.DataFrame({'review': neg_texts, 'label': neg_labels}, columns=['review', 'label'])"
      ],
      "metadata": {
        "id": "zCmPCS_fJ4Hv"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_train = pos_dataset.sample(frac = 0.8)\n",
        "neg_train = neg_dataset.sample(frac = 0.8)\n",
        "pos_part_20 = pos_dataset.drop(pos_train.index)\n",
        "neg_part_20 = neg_dataset.drop(neg_train.index)"
      ],
      "metadata": {
        "id": "vUYRi-5BKvxv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_test = pos_part_20.sample(frac = 0.5)\n",
        "neg_test = neg_part_20.sample(frac = 0.5)\n",
        "pos_val = pos_part_20.drop(pos_test.index)\n",
        "neg_val = neg_part_20.drop(neg_test.index)"
      ],
      "metadata": {
        "id": "oo79Ca12Lvcx"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set= pd.concat([pos_train, neg_train], axis=0)\n",
        "test_set=pd.concat([pos_test, neg_test], axis=0)\n",
        "val_set=pd.concat([pos_val, neg_val], axis=0)\n",
        "dataset =pd.concat([train_set, test_set,val_set], axis=0)"
      ],
      "metadata": {
        "id": "af5eKV23LWMZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = train_set.reset_index()\n",
        "test_set = test_set.reset_index()\n",
        "val_set = val_set.reset_index()\n",
        "dataset = dataset.reset_index()"
      ],
      "metadata": {
        "id": "dHiF0XnlNG0v"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Raw data: ')\n",
        "print('max length =',np.max([len(x) for x in dataset['review']]))\n",
        "print('mean length =',np.mean([len(x) for x in dataset['review']]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mU2I1NNbOljf",
        "outputId": "e08d9b5f-ce9d-4837-9531-ccf86bea7401"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw data: \n",
            "max length = 13704\n",
            "mean length = 1309.43102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word.isalpha() and word not in stop_words]\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "train_set['review'] = train_set['review'].apply(normalize_text)\n",
        "val_set['review'] = val_set['review'].apply(normalize_text)\n",
        "test_set['review'] = test_set['review'].apply(normalize_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwAW9NbfSeNM",
        "outputId": "ce4b2185-bbb7-4a3a-d568-5f8f11262783"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_v2 =pd.concat([train_set, test_set,val_set], axis=0)\n",
        "\n",
        "print('After normalization: ')\n",
        "print('max length =',np.max([len(x) for x in dataset_v2['review']]))\n",
        "print('mean length =',np.mean([len(x) for x in dataset_v2['review']]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxGTv6JUTeia",
        "outputId": "2f95aeeb-6d89-4bc7-d71c-1bff0121f75c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After normalization: \n",
            "max length = 9164\n",
            "mean length = 812.165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "FNKHqH6Pj18N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0952858e-6e5a-47a2-e8f1-37f3fe78ac85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After vectorization: \n",
            "max length = 1383\n",
            "mean length = 119.72716\n"
          ]
        }
      ],
      "source": [
        "# Import the necessary libraries\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create the vocabulary from the training dataset\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=30000)\n",
        "tokenizer.fit_on_texts(train_set['review'])\n",
        "vocab_size= len(tokenizer.word_index)+1\n",
        "\n",
        "# Encode the text data as sequences of integers\n",
        "x_train = tokenizer.texts_to_sequences(train_set['review'])\n",
        "x_val = tokenizer.texts_to_sequences(val_set['review'])\n",
        "x_test = tokenizer.texts_to_sequences(test_set['review'])\n",
        "\n",
        "dataset_v3 = x_train + x_val + x_test\n",
        "\n",
        "print('After vectorization: ')\n",
        "print('max length =',np.max([len(x) for x in dataset_v3]))\n",
        "print('mean length =',np.mean([len(x) for x in dataset_v3]))\n",
        "\n",
        "# Pad the sequences to the same length\n",
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=500)\n",
        "x_val = tf.keras.preprocessing.sequence.pad_sequences(x_val, maxlen=500)\n",
        "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=500)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('After padding: ')\n",
        "print('max length =',np.max([len(x) for x in x_train]))\n",
        "print('mean length =',np.mean([len(x) for x in x_train]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkzNmuUvKRke",
        "outputId": "7fdf0a35-b93e-459f-c044-e951c341b338"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After padding: \n",
            "max length = 500\n",
            "mean length = 500.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Fit and transform the label data\n",
        "y_train = le.fit_transform(train_set['label'])\n",
        "y_val = le.transform(val_set['label'])\n",
        "y_test = le.transform(test_set['label'])\n",
        "\n",
        "# Convert the labels to categorical data\n",
        "y_train = tf.keras.utils.to_categorical(y_train)\n",
        "y_val = tf.keras.utils.to_categorical(y_val)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)"
      ],
      "metadata": {
        "id": "FrvBtoZUPOZJ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, 100, input_length=500))\n",
        "model.add(tf.keras.layers.LSTM(256))\n",
        "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "score = model.evaluate(x_test, y_test, batch_size=32)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHQVB218s0HW",
        "outputId": "deb2b881-543d-494c-fb82-2089e5f61d1f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1250/1250 [==============================] - 131s 104ms/step - loss: 0.3528 - accuracy: 0.8456 - val_loss: 0.2967 - val_accuracy: 0.8804\n",
            "Epoch 2/5\n",
            "1250/1250 [==============================] - 59s 47ms/step - loss: 0.2398 - accuracy: 0.9026 - val_loss: 0.3031 - val_accuracy: 0.8764\n",
            "Epoch 3/5\n",
            "1250/1250 [==============================] - 47s 38ms/step - loss: 0.1280 - accuracy: 0.9553 - val_loss: 0.3560 - val_accuracy: 0.8924\n",
            "Epoch 4/5\n",
            "1250/1250 [==============================] - 43s 35ms/step - loss: 0.0708 - accuracy: 0.9759 - val_loss: 0.3636 - val_accuracy: 0.8814\n",
            "Epoch 5/5\n",
            "1250/1250 [==============================] - 41s 33ms/step - loss: 0.0384 - accuracy: 0.9875 - val_loss: 0.5646 - val_accuracy: 0.8762\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.5520 - accuracy: 0.8814\n",
            "Test loss: 0.551957368850708\n",
            "Test accuracy: 0.8813999891281128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, 100, input_length=500))\n",
        "model.add(tf.keras.layers.Conv1D(512, 5, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(.3))\n",
        "model.add(tf.keras.layers.MaxPooling1D(5))\n",
        "model.add(tf.keras.layers.Conv1D(512, 5, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(.2))\n",
        "model.add(tf.keras.layers.MaxPooling1D(5))\n",
        "model.add(tf.keras.layers.Conv1D(512, 5, activation='relu'))\n",
        "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "score = model.evaluate(x_test, y_test, batch_size=32)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9thEic4s5O0",
        "outputId": "bb75b18a-f0ca-438c-ff8d-31913216cdda"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1250/1250 [==============================] - 92s 72ms/step - loss: 0.3314 - accuracy: 0.8489 - val_loss: 0.2861 - val_accuracy: 0.8986\n",
            "Epoch 2/5\n",
            "1250/1250 [==============================] - 48s 39ms/step - loss: 0.1595 - accuracy: 0.9408 - val_loss: 0.2869 - val_accuracy: 0.8810\n",
            "Epoch 3/5\n",
            "1250/1250 [==============================] - 41s 33ms/step - loss: 0.0730 - accuracy: 0.9742 - val_loss: 0.3494 - val_accuracy: 0.8782\n",
            "Epoch 4/5\n",
            "1250/1250 [==============================] - 37s 30ms/step - loss: 0.0366 - accuracy: 0.9873 - val_loss: 0.4966 - val_accuracy: 0.8512\n",
            "Epoch 5/5\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.0291 - accuracy: 0.9902 - val_loss: 0.4716 - val_accuracy: 0.8694\n",
            "157/157 [==============================] - 1s 8ms/step - loss: 0.5008 - accuracy: 0.8652\n",
            "Test loss: 0.5007619857788086\n",
            "Test accuracy: 0.8651999831199646\n"
          ]
        }
      ]
    }
  ]
}