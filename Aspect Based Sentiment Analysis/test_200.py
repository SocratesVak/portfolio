# -*- coding: utf-8 -*-
"""test_200

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/156JSeqzld29AprFvKmFYfOU_6e_ShRAT
"""

import pandas as pd
import numpy as np
import pickle
import nltk
import glob
import re

from nltk.stem import WordNetLemmatizer
from sklearn.feature_selection import VarianceThreshold,SelectKBest,chi2
from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn import svm, metrics
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk import pos_tag
nltk.download('averaged_perceptron_tagger')



def test200(array):
    """φορτώνουμε τα μοντέλα που παρήχθησαν από την εκπαίδευση στο προηγούμενο script, ώστε να τα αξιοποίησουμε για την πρόβλεψη σε δεδομένα,
    πάνω στα οποία δεν έχει εκπαιδευτεί"""

    model_pol = pickle.load(open('model_polarity','rb'))
    model_asp = pickle.load(open('model_aspect','rb'))


    """ακολουθούμε την ίδια διαδικασία, που εφαρμόσαμε και στο train, ώστε να φέρουμε τα δεδομένα του αγνώστου κειμένου σε μια κατά το δυνατόν κοντινή μορφή 
    με εκείνη των δεδομένων εκπαίδευσης."""

    path = '.'
    filenames = glob.glob(path + f'/part{array}.xml')
    # Create empty variable, read data from all the xml files, concatenate them
    read_df = []
    for filename in filenames:
        read_df.append(pd.read_xml(filename, xpath=".//sentence|.//Opinion"))
        df = pd.concat(read_df)




    # Replace empty 'text' column with the values in the cell above
    df['text'] = df['text'].replace('', np.nan).ffill()

    #διαγράφω τις άχρηστες στήλες
    df.drop(["OutOfScope","Opinions",'target','from','to','id'], axis = 1, inplace = True)
    newdf = df.dropna()



    X_train = newdf[['text']].copy()
    y =newdf[['category','polarity']].copy()





    le=LabelEncoder()
    y['category'] = le.fit_transform(y['category'])
    le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
    #print(le_name_mapping)
    y['polarity'] = le.fit_transform(y['polarity'])
    le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
    #print(le_name_mapping)






    X_train['Review_without_stopwords']= X_train['text'].copy()

    def rem_stopwords_tokenize(data,name):
      
      def getting(sent):
          example_sent = sent
          
          filtered_sentence = [] 

          stop_words = set(stopwords.words('english')) 

          word_tokens = word_tokenize(example_sent) 
          
          filtered_sentence = [w for w in word_tokens if not w in stop_words] 
          
          return filtered_sentence

      x=[]
      for i in data[name].values:
          x.append(getting(i))
      data[name]=x

    




    def Lemmatization(data,name):
      lemmatizer = WordNetLemmatizer()

      def getting2(sent):
          
          example = sent
          output_sentence =[]
          word_tokens2 = word_tokenize(example)
          lemmatized_output = [lemmatizer.lemmatize(w) for w in word_tokens2]
          
          # Remove characters which have length less than 2  
          without_single_chr = [word for word in lemmatized_output if len(word) > 2]
          # Remove numbers
          cleaned_data_title = [word for word in without_single_chr if not word.isnumeric()]
          
          return cleaned_data_title

      x=[]
      for i in data[name].values:
          x.append(getting2(i))
      data[name]=x

      


      
    def pos_tagging(data,name):

        def getting3(sen):
          example=sen
          tokens=word_tokenize(example)
          sent_tagged=nltk.pos_tag(tokens)
          return sent_tagged

        x=[]
        for i in data[name].values:
          x.append(getting3(i))

        new_text_pos_tagged=[]
        for sent in x:
          new_sent=[]
          for word in sent:
            new_word= word[0] + "/" + word[1]
            new_sent.append(new_word)
          new_text_pos_tagged.append(new_sent)

        data[name]=new_text_pos_tagged



    def make_sentences(data,name):
      data[name]=data[name].apply(lambda x:' '.join([i+' ' for i in x]))
      # Removing double spaces if created
      data[name]=data[name].apply(lambda x:re.sub(r'\s+', ' ', x, flags=re.I))






    rem_stopwords_tokenize(X_train,name='Review_without_stopwords')
    make_sentences(X_train,'Review_without_stopwords')


    X_train['After_Lemmatization']=X_train['Review_without_stopwords'].copy()
    X_train.drop(["Review_without_stopwords"], axis = 1, inplace = True)
    Lemmatization(X_train,name='After_Lemmatization')
    make_sentences(X_train,'After_Lemmatization')


    X_train['POS_tags']=X_train['After_Lemmatization'].copy()
    pos_tagging(X_train,name='POS_tags')
    make_sentences(X_train,'POS_tags')
    




    tf_idf = TfidfVectorizer(lowercase=True, tokenizer=word_tokenize, ngram_range=(1, 3), max_features=5000) 

    X_text_test = tf_idf.fit_transform(X_train['text'])
    X_text_test = pd.DataFrame.sparse.from_spmatrix(X_text_test)
    X_lemma_test= tf_idf.fit_transform(X_train['After_Lemmatization'])
    X_lemma_test= pd.DataFrame.sparse.from_spmatrix(X_lemma_test)
    X_pos_test= tf_idf.transform(X_train['POS_tags'])
    X_pos_test= pd.DataFrame.sparse.from_spmatrix(X_pos_test)



    X = pd.concat([X_text_test, X_lemma_test, X_pos_test], axis=1, join='inner')
    X_new = X.to_numpy()   
    



    """Εδώ, όπως και στην εκπαίδευση, θα χρησιμοποιήσουμε τα 200 χρησιμότερα features που έβγαλε ο vectorizer. Στη συνέχεια, βάζουμε το μοντέλο, που έχουμε
    εκπαιδεύσει σε άλλα δεδομένα, να προβεί σε πρόβλεψη για το polarity σε δεδομένα, που δεν έχει ξαναδεί. Αξιολογούμε την πρόβλεψη με βάση το accuracy score """
    
    X_new_pol = SelectKBest(chi2, k=200).fit_transform(X_new, y['polarity'])
    y_pred_pol= model_pol.predict(X_new_pol)
    accuracy_polarity=metrics.accuracy_score(y['polarity'], y_pred_pol)
  
    """ Εφαρμόζουμε το ίδιο και για την πρόβλεψη του aspect χρησιμοποιώντας το μοντέλο που είχε εκπαιδευτεί στο ανάλογο task"""

    X_new_asp = SelectKBest(chi2, k=200).fit_transform(X_new, y['category'])
    y_pred_asp= model_asp.predict(X_new_asp)
    accuracy_aspect= metrics.accuracy_score(y['category'], y_pred_asp)
    

    """η συνάρτηση test θέλουμε να επιστρέφει τα accuracy score για το polarity και τα aspect, ώστε να τα αξιοποιήσουμε
    στο επόμενο στάδιο"""


    return accuracy_polarity,accuracy_aspect