# -*- coding: utf-8 -*-
"""Word2Vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tBBcvv2EN4k06TmN2yMUi978YANEMKMi
"""

from google.colab import drive
drive.mount('/content/drive')
#συνδέομαι με το google drive
import os 
os.chdir('/content/drive/MyDrive/')
!pwd #εντολή που μας τυπώνει σε ποιο φάκελο κοιτάει ο υπολογιστής

with open('./europarl_en.txt', 'r', encoding='utf-8') as text_eng:
    lines_eng = text_eng.read().splitlines()

print("Number of original sentences:",len(lines_eng))
en_unique = set(lines_eng) # remove duplicate
print("Number of sentences after the removal of duplicates:",len(en_unique))
en_unique_list = list(en_unique)

import re
# using findall() to neglect unicode of Non-English alphabets
sentences = [sent for sent in en_unique_list if not re.findall("[^\u0000-\u05C0\u2100-\u214F]+", sent)]

import random 
reduction_rate = int(len(sentences)/4)
final_sentences= random.sample(sentences, reduction_rate)
print("Number of sentences after the randomly done reduction of the corpus size:",reduction_rate)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import gensim
# # Track the total number of tokens in the dataset.
# num_tokens = 0
# # List of sentences to use for training.
# sentences = []
# for sentence in final_sentences:
# 
#     # Tokenize the comment. This returns a list of words.
#     parsed = gensim.utils.simple_preprocess(sentence)
# 
#     # Accumulate the total number of words in the dataset.
#     num_tokens += len(parsed)
# 
#     # Add the comment to the list.
#     sentences.append(parsed)
# print("Number of tokens:",num_tokens)

model = gensim.models.Word2Vec (
    size=100,    # Number of features in word vector
    
    window=10,   # Context window size (in each direction)
                 #   Default is 5
    
    min_count=2, # Words must appear this many times to be in vocab.
                 #   Default is 5
    
    workers=10,  # Training thread count
    
    sg=0,        # 0: CBOW, 1: Skip-gram. 
                 #   Default is 0, CBOW
    
    hs=0,        # 0: Negative Sampling, 1: Hierarchical Softmax
                 #   Default is 0, NS
    
    negative=2,   # Number of negative samples 
                 
    sample=0.001,

    ns_exponent=0.75
)

# Build the vocabulary using the comments in "sentences".
model.build_vocab(sentences)

import pandas as pd
# Sort the word list by frequency.
words = sorted(model.wv.index2word, key=lambda word: model.wv.vocab[word].count, reverse=True)

top_words = []
for i in range(48):
    # Add the word and it's count (formatted with commas)
    top_words.append((words[i], '{:,}'.format(model.wv.vocab[words[i]].count)))

df = pd.DataFrame(top_words, columns=['Word', 'Count'])

display(df)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# print('Training the model...')
# 
# model.train(sentences,total_examples=len(sentences),epochs=5)
# 
# print('  Done.')
# print('')

model.wv.most_similar('greece')

model.wv.most_similar('facebook')

pairs = [
    ('greece', 'germany'),   
    ('greece', 'austria'),   
    ('greece', 'america'),  
    ('greece', 'iran'),   
    ('greece', 'yemen') ]

for w1, w2 in pairs:
    print('%r\t%r\t%.2f' % (w1, w2, model.wv.similarity(w1, w2)))

pairs = [
    ('orange', 'fruit'),   
    ('orange', 'banana'),   
    ('orange', 'apple'),     
    ('orange', 'car') ]

for w1, w2 in pairs:
    print('%r\t%r\t%.2f' % (w1, w2, model.wv.similarity(w1, w2)))

print(model.wv.doesnt_match("austria germany greece iran".split()))

print(model.wv.doesnt_match("orange banana car apple ".split()))

results = model.wv.most_similar(positive=['greece', 'iran'], negative=['europe'])
for result in results:
  print(result)

results = model.wv.most_similar(positive=['woman', 'father'], negative=['man'])
for result in results:
  print(result)

from nltk.corpus import stopwords
from nltk import download
stop_words = set(stopwords.words("english"))



sentence_austria = 'Poettering speaks to the media in Germany'.lower().split()
sentence_austria = [w for w in sentence_austria if w not in stop_words]


sentence_president = 'The Austrian president talks to the press in Berlin'.lower().split()
sentence_president = [w for w in sentence_president if w not in stop_words]

sentence_orange= "Oranges are my favorite fruit".lower().split()
sentence_orange = [w for w in sentence_orange if w not in stop_words]

model.wv.init_sims(replace=True) 

distance = model.wv.wmdistance(sentence_austria, sentence_president)
print('For the first two sentences the distance = %.4f' % distance)

distance = model.wv.wmdistance(sentence_president, sentence_orange)
print('For the last two sentences the distance = %.4f' % distance)