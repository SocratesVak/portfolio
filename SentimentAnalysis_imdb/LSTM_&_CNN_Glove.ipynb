{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar xvzf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "id": "7IgnCs2Alh5-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "positive_samples = []\n",
        "positive_directory = '/content/aclImdb/train/pos/'\n",
        "for filename in os.listdir(positive_directory):\n",
        "    with open(os.path.join(positive_directory, filename), 'r') as file:\n",
        "        positive_samples.append(file.read())"
      ],
      "metadata": {
        "id": "c4z6L8_tuGyD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_directory = '/content/aclImdb/test/pos/'\n",
        "for filename in os.listdir(positive_directory):\n",
        "    with open(os.path.join(positive_directory, filename), 'r') as file:\n",
        "        positive_samples.append(file.read())"
      ],
      "metadata": {
        "id": "klxJ2qo-TVtX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_samples = []\n",
        "negative_directory = '/content/aclImdb/train/neg/'\n",
        "for filename in os.listdir(negative_directory):\n",
        "    with open(os.path.join(negative_directory, filename), 'r') as file:\n",
        "        negative_samples.append(file.read())"
      ],
      "metadata": {
        "id": "P9jiHPVoTVtY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_directory = '/content/aclImdb/test/neg/'\n",
        "for filename in os.listdir(negative_directory):\n",
        "    with open(os.path.join(negative_directory, filename), 'r') as file:\n",
        "        negative_samples.append(file.read())"
      ],
      "metadata": {
        "id": "3mdNhyYzOzf3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "pos_texts= np.array(positive_samples)\n",
        "neg_texts= np.array(negative_samples)\n",
        "pos_labels=  np.array([1]*len(positive_samples))\n",
        "neg_labels=  np.array([0]*len(negative_samples))\n",
        "\n",
        "pos_dataset = pd.DataFrame({'review': pos_texts, 'label': pos_labels}, columns=['review', 'label'])\n",
        "neg_dataset = pd.DataFrame({'review': neg_texts, 'label': neg_labels}, columns=['review', 'label'])"
      ],
      "metadata": {
        "id": "zCmPCS_fJ4Hv"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_train = pos_dataset.sample(frac = 0.8)\n",
        "neg_train = neg_dataset.sample(frac = 0.8)\n",
        "pos_part_20 = pos_dataset.drop(pos_train.index)\n",
        "neg_part_20 = neg_dataset.drop(neg_train.index)"
      ],
      "metadata": {
        "id": "vUYRi-5BKvxv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_test = pos_part_20.sample(frac = 0.5)\n",
        "neg_test = neg_part_20.sample(frac = 0.5)\n",
        "pos_val = pos_part_20.drop(pos_test.index)\n",
        "neg_val = neg_part_20.drop(neg_test.index)"
      ],
      "metadata": {
        "id": "oo79Ca12Lvcx"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set= pd.concat([pos_train, neg_train], axis=0)\n",
        "test_set=pd.concat([pos_test, neg_test], axis=0)\n",
        "val_set=pd.concat([pos_val, neg_val], axis=0)\n",
        "dataset =pd.concat([train_set, test_set,val_set], axis=0)"
      ],
      "metadata": {
        "id": "af5eKV23LWMZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = train_set.reset_index()\n",
        "test_set = test_set.reset_index()\n",
        "val_set = val_set.reset_index()\n",
        "dataset = dataset.reset_index()"
      ],
      "metadata": {
        "id": "dHiF0XnlNG0v"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Raw data: ')\n",
        "print('max length =',np.max([len(x) for x in dataset['review']]))\n",
        "print('mean length =',np.mean([len(x) for x in dataset['review']]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mU2I1NNbOljf",
        "outputId": "a937b8a9-be91-4305-bf89-7b9b3ab8c168"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw data: \n",
            "max length = 13704\n",
            "mean length = 1309.43102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word.isalpha() and word not in stop_words]\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "train_set['review'] = train_set['review'].apply(normalize_text)\n",
        "val_set['review'] = val_set['review'].apply(normalize_text)\n",
        "test_set['review'] = test_set['review'].apply(normalize_text)"
      ],
      "metadata": {
        "id": "UwAW9NbfSeNM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d6a0480-4ef2-480a-f41a-1d9c82f96b45"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_v2 =pd.concat([train_set, test_set,val_set], axis=0)\n",
        "\n",
        "print('After normalization: ')\n",
        "print('max length =',np.max([len(x) for x in dataset_v2['review']]))\n",
        "print('mean length =',np.mean([len(x) for x in dataset_v2['review']]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxGTv6JUTeia",
        "outputId": "2d4c7639-b1f1-4fb8-f397-cae6c8a09bcf"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After normalization: \n",
            "max length = 9164\n",
            "mean length = 812.165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create the vocabulary from the training dataset\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=30000)\n",
        "tokenizer.fit_on_texts(train_set['review'])\n",
        "\n",
        "# Encode the text data as sequences of integers\n",
        "x_train = tokenizer.texts_to_sequences(train_set['review'])\n",
        "x_val = tokenizer.texts_to_sequences(val_set['review'])\n",
        "x_test = tokenizer.texts_to_sequences(test_set['review'])\n",
        "\n",
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=500)\n",
        "x_val = tf.keras.preprocessing.sequence.pad_sequences(x_val, maxlen=500)\n",
        "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=500)"
      ],
      "metadata": {
        "id": "DTtlfL7UJG48"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('After padding: ')\n",
        "print('max length =',np.max([len(x) for x in x_train]))\n",
        "print('mean length =',np.mean([len(x) for x in x_train]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuYaIlP_UcOl",
        "outputId": "9b1a8f09-9592-4d2b-a262-ea72394f3b6f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After padding: \n",
            "max length = 500\n",
            "mean length = 500.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Fit and transform the label data\n",
        "y_train = le.fit_transform(train_set['label'])\n",
        "y_val = le.transform(val_set['label'])\n",
        "y_test = le.transform(test_set['label'])\n",
        "\n",
        "# Convert the labels to categorical data\n",
        "y_train = tf.keras.utils.to_categorical(y_train)\n",
        "y_val = tf.keras.utils.to_categorical(y_val)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)"
      ],
      "metadata": {
        "id": "FrvBtoZUPOZJ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the pre-trained word embeddings\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "id": "OEPUuiZOzLUB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained word embeddings into a dictionary\n",
        "embeddings_index = {}\n",
        "with open('glove.6B.100d.txt') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "vocab=tokenizer.sequences_to_texts(x_train)\n",
        "vocab_size=len(tokenizer.word_index)+1\n",
        "\n",
        "print('unique words: ',vocab_size)\n",
        "\n",
        "# Create an embedding matrix to use in the model\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < vocab_size:\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "      break"
      ],
      "metadata": {
        "id": "2eTTHfmhyW9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed6f8145-13d8-4068-d131-c3b5a340b84f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unique words:  81821\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=500, trainable=False))\n",
        "model.add(tf.keras.layers.LSTM(256))\n",
        "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "score = model.evaluate(x_test, y_test, batch_size=32)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHQVB218s0HW",
        "outputId": "2ffc57a3-926f-41e2-9a29-44014f8be23e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1250/1250 [==============================] - 39s 26ms/step - loss: 0.4548 - accuracy: 0.7893 - val_loss: 0.3930 - val_accuracy: 0.8370\n",
            "Epoch 2/5\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.3311 - accuracy: 0.8577 - val_loss: 0.3387 - val_accuracy: 0.8504\n",
            "Epoch 3/5\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.3005 - accuracy: 0.8740 - val_loss: 0.3124 - val_accuracy: 0.8656\n",
            "Epoch 4/5\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.2708 - accuracy: 0.8874 - val_loss: 0.2976 - val_accuracy: 0.8736\n",
            "Epoch 5/5\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.2395 - accuracy: 0.9018 - val_loss: 0.2907 - val_accuracy: 0.8770\n",
            "157/157 [==============================] - 2s 12ms/step - loss: 0.2809 - accuracy: 0.8850\n",
            "Test loss: 0.2808544337749481\n",
            "Test accuracy: 0.8849999904632568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=500, trainable=False))\n",
        "model.add(tf.keras.layers.Conv1D(512, 5, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(.3))\n",
        "model.add(tf.keras.layers.MaxPooling1D(5))\n",
        "model.add(tf.keras.layers.Conv1D(512, 5, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(.2))\n",
        "model.add(tf.keras.layers.MaxPooling1D(5))\n",
        "model.add(tf.keras.layers.Conv1D(512, 5, activation='relu'))\n",
        "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "score = model.evaluate(x_test, y_test, batch_size=32)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9thEic4s5O0",
        "outputId": "56a3eead-dfdc-4942-e1e6-45aee02e73bd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1250/1250 [==============================] - 31s 20ms/step - loss: 0.4400 - accuracy: 0.7939 - val_loss: 0.3739 - val_accuracy: 0.8402\n",
            "Epoch 2/5\n",
            "1250/1250 [==============================] - 25s 20ms/step - loss: 0.3347 - accuracy: 0.8561 - val_loss: 0.3351 - val_accuracy: 0.8602\n",
            "Epoch 3/5\n",
            "1250/1250 [==============================] - 25s 20ms/step - loss: 0.2933 - accuracy: 0.8766 - val_loss: 0.3267 - val_accuracy: 0.8602\n",
            "Epoch 4/5\n",
            "1250/1250 [==============================] - 25s 20ms/step - loss: 0.2538 - accuracy: 0.8953 - val_loss: 0.3178 - val_accuracy: 0.8612\n",
            "Epoch 5/5\n",
            "1250/1250 [==============================] - 25s 20ms/step - loss: 0.2117 - accuracy: 0.9139 - val_loss: 0.3667 - val_accuracy: 0.8408\n",
            "157/157 [==============================] - 1s 8ms/step - loss: 0.3651 - accuracy: 0.8410\n",
            "Test loss: 0.3651462197303772\n",
            "Test accuracy: 0.8410000205039978\n"
          ]
        }
      ]
    }
  ]
}